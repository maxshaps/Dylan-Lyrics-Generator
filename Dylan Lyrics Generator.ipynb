{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dylan Lyrics Generator.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNxBubCzdsxCU87uDKGL0WQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uaZE0huja9hS","colab_type":"code","outputId":"43e813f6-0dee-4e77-e3b1-00b65534832f","executionInfo":{"status":"ok","timestamp":1584238392958,"user_tz":420,"elapsed":18012,"user":{"displayName":"Max Shapiro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9NYDzrXDm_jD6soPGUtStPeWn3HdIpQfMUDz=s64","userId":"03227170244121789030"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cX-qcTFQbEng","colab_type":"code","outputId":"4e0e9bd0-f9d9-4376-8391-de19c07b4595","executionInfo":{"status":"ok","timestamp":1584240307506,"user_tz":420,"elapsed":1165,"user":{"displayName":"Max Shapiro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9NYDzrXDm_jD6soPGUtStPeWn3HdIpQfMUDz=s64","userId":"03227170244121789030"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Checking to see if directories on my Google Drive are accessible\n","!ls '/content/gdrive/My Drive/Dylan Lyrics Generator'"],"execution_count":1,"outputs":[{"output_type":"stream","text":[" dylan_corpus.txt  'Dylan Lyrics Generator.ipynb'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tCjLIlknbG-R","colab_type":"code","outputId":"b5e4b2df-dd90-43c6-caf0-782398914183","executionInfo":{"status":"ok","timestamp":1584240312838,"user_tz":420,"elapsed":2641,"user":{"displayName":"Max Shapiro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9NYDzrXDm_jD6soPGUtStPeWn3HdIpQfMUDz=s64","userId":"03227170244121789030"}},"colab":{"base_uri":"https://localhost:8080/","height":175}},"source":["# Installing PyTorch torchtext\n","!pip3 install torch torchtext"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"z3eNCUzFbJRv","colab_type":"code","outputId":"9bed73d5-24e1-4aa6-9997-11a589f91da4","executionInfo":{"status":"ok","timestamp":1584240315644,"user_tz":420,"elapsed":429,"user":{"displayName":"Max Shapiro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9NYDzrXDm_jD6soPGUtStPeWn3HdIpQfMUDz=s64","userId":"03227170244121789030"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# Make sure PyTorch and GPU training are available\n","import torch\n","print(torch.cuda.is_available())\n","print(torch.backends.cudnn.version())\n","print(torch.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["True\n","7603\n","1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-w0p3tJ6bLYI","colab_type":"code","colab":{}},"source":["# Importing packages\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from collections import Counter\n","import os\n","import string\n","from argparse import Namespace\n","\n","# File location, parameters of LSTM model, properties of text corpus, etc.\n","flags = Namespace(train_file='/content/gdrive/My Drive/Dylan Lyrics Generator/dylan_corpus.txt',\n","                  seq_size=100,\n","                  batch_size=128,\n","                  embedding_size=256,\n","                  lstm_size=256,\n","                  gradients_norm=5,\n","                  initial_words=['well', 'i'],\n","                  predict_top_k=5,\n","                  checkpoint_path='checkpoint',\n","                  )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOFVWQMNcJOF","colab_type":"code","colab":{}},"source":["# Load and process text data\n","def get_data_from_file(train_file, batch_size, seq_size):\n","  # Read data from file\n","  with open(train_file, 'r') as f:\n","    text = f.read()\n","  \n","  # Convert text to lowercase, tokenize, and remove punctuation\n","  text = text.lower().split()\n","  text = [i.replace(',','').replace('.','').replace('!','').replace('?','').replace(':','') for i in text if i not in string.punctuation]\n","  \n","  # Create two dictionaries:\n","  # 1) int_to_vocab, which converts word tokens into integer indices\n","  # 2) vocab_to_int, which converts integer indices back to word tokens\n","  word_counts = Counter(text)\n","  sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n","  int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n","  vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n","  n_vocab = len(int_to_vocab)\n","  \n","  # Convert word tokens into integer indices\n","  int_text = [vocab_to_int[w] for w in text]\n","  num_batches = int(len(int_text) / (seq_size * batch_size))\n","  in_text = int_text[:num_batches * batch_size * seq_size] # Drop suitable amount of data so mini-batches divide evenly\n","  \n","  # Create target data\n","  # Target of each input word is its consecutive word\n","  out_text = np.zeros_like(in_text)\n","  out_text[:-1] = in_text[1:]\n","  out_text[-1] = in_text[0]\n","\n","  # Reshape into tensors suitable for LTSM input / training\n","  in_text = np.reshape(in_text, (batch_size, -1))\n","  out_text = np.reshape(out_text, (batch_size, -1))\n","  \n","  return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text\n","\n","\n","# Partition data into batches, yielding iterable generators\n","def get_batches(in_text, out_text, batch_size, seq_size):\n","  num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n","  \n","  for i in range(0, num_batches * seq_size, seq_size):\n","    yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6UtLz0tM5MC","colab_type":"code","outputId":"7782596e-0bed-4ea1-f0c0-42022212c163","executionInfo":{"status":"ok","timestamp":1584240326939,"user_tz":420,"elapsed":404,"user":{"displayName":"Max Shapiro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9NYDzrXDm_jD6soPGUtStPeWn3HdIpQfMUDz=s64","userId":"03227170244121789030"}},"colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["# Summarize vocab size and data parameters\n","int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(train_file=flags.train_file,\n","                                                                            batch_size=flags.batch_size,\n","                                                                            seq_size=flags.seq_size)\n","\n","print('Vocabulary Size: {}'.format(n_vocab))\n","print('in_text Shape: {}'.format(in_text.shape))\n","print('out_text Shape: {}'.format(out_text.shape))\n","print(in_text[:10,:10])\n","print(out_text[:10,:10])\n","print('10 Random Words In Vocab: {}'.format([int_to_vocab[i] for i in np.random.randint(0,n_vocab,10)]))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 8781\n","in_text Shape: (128, 1300)\n","out_text Shape: (128, 1300)\n","[[  31    3   26   32  211    3   59    1   21    3]\n"," [ 278   13  852 1842    2 5582    2  950   19   87]\n"," [  25 4186    9   61   19  142  179  111 2280 2017]\n"," [  17 3442   68    0 1856   40 1262   68  255   17]\n"," [   7    0   65   16   19    0  141   67   15   19]\n"," [  33   62    0 1717    4 4253    2   11    0  337]\n"," [ 108   12   65   77 5754  544   76 5755   41    1]\n"," [   3  214  127   55   17   78   15  302   22    1]\n"," [ 966    5  881 2078  329  143 1899   46   37 1735]\n"," [ 116 1480   25  680   74  231   44   14    9    4]]\n","[[   3   26   32  211    3   59    1   21    3   55]\n"," [  13  852 1842    2 5582    2  950   19   87 2548]\n"," [4186    9   61   19  142  179  111 2280 2017    6]\n"," [3442   68    0 1856   40 1262   68  255   17    0]\n"," [   0   65   16   19    0  141   67   15   19  156]\n"," [  62    0 1717    4 4253    2   11    0  337   11]\n"," [  12   65   77 5754  544   76 5755   41    1  371]\n"," [ 214  127   55   17   78   15  302   22    1    3]\n"," [   5  881 2078  329  143 1899   46   37 1735  801]\n"," [1480   25  680   74  231   44   14    9    4 1906]]\n","10 Random Words In Vocab: ['\"\"why', 'a-broke', 'intense', 'stroke', 'anything', 'twins', 'leadbelly', 'pure', 'cut', 'night-sticks']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9-bSO9HUcx69","colab_type":"code","colab":{}},"source":["class RNNModule(nn.Module):\n","\n","  # Set up model architecture, consisting of an embedding layer, a two-layer LSTM with p=0.2 dropout, and a fully connected layer\n","  def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n","    super(RNNModule, self).__init__()\n","    self.seq_size = seq_size\n","    self.embedding_size = embedding_size\n","    self.lstm_size = lstm_size\n","    \n","    self.embedding = nn.Embedding(n_vocab, embedding_size)\n","    self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True, num_layers=2, dropout=0.2)\n","    self.dense = nn.Linear(lstm_size, n_vocab)\n","  \n","\n","  # Constructing the forward pass\n","  def forward(self, x, prev_state):\n","    embed = self.embedding(x)\n","    output, state = self.lstm(embed, prev_state)\n","    output = self.dense(output)\n","    \n","    return output, state\n","  \n","\n","  # Helper method that zeros hidden and cell state tensors\n","  # Called to initialize and reset tensors after each epoch\n","  def zero_state(self, batch_size):\n","    return (torch.zeros(2, batch_size, self.lstm_size), torch.zeros(2, batch_size, self.lstm_size))\n","  \n","  # Make lyrics predictions given initial words\n","  def predict(model, words, n_vocab, vocab_to_int, int_to_vocab, output_size, top_k=5):\n","    # Setting to evaluation mode\n","    model.eval()\n","    \n","    # Initialize hidden and cell state tensors and move to GPU\n","    state_h, state_c = model.zero_state(1)\n","    state_h = state_h.cuda()\n","    state_c = state_c.cuda()\n","    \n","    # For each input word, convert to indices and then feed to model\n","    for w in words:\n","      ix = torch.tensor([[vocab_to_int[w]]]).cuda()\n","      output, (state_h, state_c) = model(ix, (state_h, state_c))\n","\n","    # Return indices of k best output words and then randomly choose one  \n","    _, top_ix = torch.topk(output[0], k=top_k)\n","    choices = top_ix.tolist()\n","    choice = np.random.choice(choices[0])\n","    words.append(int_to_vocab[choice])\n","\n","    # Use previously chosen word as initial input for subsequently generated words\n","    for _ in range(output_size-len(words)):\n","      ix = torch.tensor([[choice]]).cuda()\n","      output, (state_h, state_c) = model(ix, (state_h, state_c))\n","      \n","      _, top_ix = torch.topk(output[0], k=top_k)\n","      choices = top_ix.tolist()\n","      choice = np.random.choice(choices[0])\n","      words.append(int_to_vocab[choice])\n","\n","    print(' '.join(words))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Tu5psaPdVbl","colab_type":"code","colab":{}},"source":["# Load and process text data\n","int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(train_file=flags.train_file,\n","                                                                            batch_size=flags.batch_size,\n","                                                                            seq_size=flags.seq_size)\n","\n","# Instantiate LSTM and transfer to GPU\n","model = RNNModule(n_vocab=n_vocab, seq_size=flags.seq_size, embedding_size=flags.embedding_size, lstm_size=flags.lstm_size)\n","model = model.cuda()\n","\n","# Hyperparameters\n","learning_rate = 1e-3\n","l2_reg_strength = 3e-4\n","num_epochs = 50\n","\n","# Use cross entropy loss and Adam optimizer with L2 regularization\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = optim.Adam(params=model.parameters(), lr=learning_rate, weight_decay=l2_reg_strength)\n","\n","for epoch in range(num_epochs):\n","  # Load data batches and initialize hidden/cell state tensors\n","  batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n","  state_h, state_c = model.zero_state(flags.batch_size)\n","\n","  # Transfer data to GPU\n","  state_h = state_h.cuda()\n","  state_c = state_c.cuda()\n","  \n","  for x, y in batches:    \n","    # Set to training mode\n","    model.train()\n","    \n","    # Reset all gradients\n","    optimizer.zero_grad()\n","    \n","    # Transfer data to GPU\n","    x = torch.tensor(x).cuda()\n","    y = torch.tensor(y).cuda()\n","    \n","    # Compute output and loss\n","    output, (state_h, state_c) = model(x, (state_h, state_c))\n","    loss = loss_fn(output.transpose(1, 2), y)\n","    \n","    # Detach ensures success of backprop\n","    state_h = state_h.detach()\n","    state_c = state_c.detach()\n","    \n","    # Perform backpropagation\n","    loss.backward()\n","\n","    # Gradient clipping helps eschew exploding gradient problem\n","    _ = torch.nn.utils.clip_grad_norm_(model.parameters(), flags.gradients_norm)\n","    \n","    # Update the network's parameters\n","    optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fH3RxyZtoXz","colab_type":"code","outputId":"d81d90ef-88ab-40f3-d75e-2b0dae854ea5","executionInfo":{"status":"ok","timestamp":1584240570857,"user_tz":420,"elapsed":352,"user":{"displayName":"Max Shapiro","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjh9NYDzrXDm_jD6soPGUtStPeWn3HdIpQfMUDz=s64","userId":"03227170244121789030"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Use optimized model to generate Dylan-like lyrics!\n","RNNModule.predict(model=model, words=['tangled','up','in'],\n","                  n_vocab=n_vocab, vocab_to_int=vocab_to_int,\n","                  int_to_vocab=int_to_vocab, output_size=30, top_k=5)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tangled up in my man i don't do it and it don't know i ain't be to serve i know you have to be the way but you can't see\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WlZ4Mbybv4XR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}